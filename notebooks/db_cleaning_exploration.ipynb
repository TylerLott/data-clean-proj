{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA EXPLORATION**\n",
    "---\n",
    "This notebook goes through some of the fields we want to clean. We hope to identify some patterns we can use to clean the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ESTABLISHMENT'S NAME**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# dirty database\n",
    "db_dirty = sqlite3.connect('../data/dirty_data/dirty_food_inspections.db')\n",
    "cur = db_dirty.cursor()\n",
    "\n",
    "# clean database\n",
    "db_clean = sqlite3.connect('../data/clean_data/food_inspections.db')\n",
    "cur = db_clean.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **OVERVIEW**\n",
    "In the dataset, the establishment's names have issues with consistency. Many establishments with the same name are spelled differently or have uncessary information attached to it's name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dirty Data**\n",
    "\n",
    "The query shows the different forms the establishment **SUBWAY** can be respresented. Some differences include:\n",
    "\n",
    "* **address information** (e.g. LINCOLN AVE SUBWAY on row 6)\n",
    "\n",
    "* **number values** (e.g. SUBWAY #38326 on row 10)\n",
    "\n",
    "* **capitolization differences** (e.g. Lakeview Subway on row 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = '''\n",
    "SELECT estName as Name, COUNT(estName) as 'num of establishments'\n",
    "FROM Establishments\n",
    "WHERE Name LIKE '%subway%'\n",
    "GROUP BY Name\n",
    "'''\n",
    "cur.execute(Q)\n",
    "df = pd.read_sql_query(Q, db_dirty)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AFTER**\n",
    "\n",
    "With the use of Open Refine, the different spellings of **SUBWAY** can be grouped under one. \n",
    "\n",
    "The query now shows how **SUBWAY** is represented in the dataset *after* cleaning in Open Refine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = '''\n",
    "SELECT estName as Name, COUNT(estName) as 'num of establishments'\n",
    "FROM Establishments\n",
    "WHERE Name LIKE '%subway'\n",
    "GROUP BY Name\n",
    "'''\n",
    "cur.execute(Q)\n",
    "df = pd.read_sql_query(Q, db_clean)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **VIOLATIONS**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import dirty data\n",
    "food_ins_df = pd.read_csv(\"../data/dirty_data/Food_Inspections.csv\")\n",
    "food_ins_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **OVERVIEW**\n",
    "The Violations columns contains a lot of unstructured information. There are *three* distinct parts to each violation.\n",
    "\n",
    "1. Health Code Violation Number\n",
    "\n",
    "2. Violation Description\n",
    "\n",
    "3. Comments\n",
    "\n",
    "**EXAMPLE**\n",
    "\n",
    "    8. sanitizing rinse for equipment and utensils:  clean, proper temperature, concentration, exposure time - comments: no dish washing facilities on site, (no three compartment sink, with grease trap, or dishmachine), instructed to provide, | 11. adequate number, convenient, accessible, designed, and maintained - comments:  no exposed hand sink for rear service area, instructed to provide, | 18. no evidence of rodent or insect outer openings protected/rodent proofed, a written log shall be maintained available to the inspectors - comments: no license pest control log book at this time of inspection, instructed to provide, upon next visit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **WEIRD CASES**\n",
    "1. Sometimes violations have pretext like 'Serious Violation <number>' (row 6) or 'Critical Violation <number>' (row 6)\n",
    "\n",
    "2. A list of old violations can appear in the comments (row 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ASSUMPTIONS**\n",
    "1. We are able to convert violations text to lowercase and lose no important meaning\n",
    "\n",
    "2. We are able to use Regex to pull the numbers. All numbers match the pattern r'\\d+\\.' (number followed by period)\n",
    "\n",
    "3. We are able to split the Violations on the number found before\n",
    "\n",
    "4. We are able to split the violations description from comments on the string '- comments:'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **ASSUMPTION 1**\n",
    "Not sure how to test this, looking through the data though I think this assumption is good. There doesn't seem to be any reason some things are all caps and others are not. I'm guessing these entries are copied directly from some PDF that is in all caps (lots of legal documents are)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **ASSUMPTION 2**\n",
    "\n",
    "**GOOD ASSUMPTION**: using the BELOW regex, we are able to capture numbers in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_assump_test = food_ins_df[['Violations']]\n",
    "total_rows = sec_assump_test.shape[0]\n",
    "sec_assump_test = sec_assump_test[sec_assump_test['Violations'].notna()]\n",
    "print(f'There are {total_rows - sec_assump_test.shape[0]} null values for Violations')\n",
    "print(f'rows left {sec_assump_test.shape[0]}')\n",
    "num_reg = r'(?:^|\\| )\\d+\\. '\n",
    "contains_num = sec_assump_test['Violations'].str.contains(num_reg, regex=True)\n",
    "print(f'Rows with numbers: {contains_num.sum()}')\n",
    "assert sec_assump_test.shape[0] == contains_num.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **ASSUMPTION 3**\n",
    "\n",
    "**GOOD ASSUMPTION**: splitting on the regex creates the same number of violation numbers and violation text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reg = r'(?:^|\\| )(\\d+\\. )'\n",
    "third_assump_test = food_ins_df[food_ins_df['Violations'].notna()][['Violations']]\n",
    "nums = third_assump_test['Violations'].str.extractall(num_reg)\n",
    "third_assump_test['v_num'] = nums.groupby(level=0)[0].apply(list)\n",
    "third_assump_test['list_violations'] = third_assump_test['Violations'].str.split(r'(?:^|\\| )\\d+\\. ', regex=True).apply(lambda x: x[1:])\n",
    "# get counts for the lists in each row for violations and violation numbers, ensure they are the same\n",
    "num_violations = third_assump_test['list_violations'].apply(lambda x: len(x))\n",
    "num_violations_nums = third_assump_test['v_num'].apply(lambda x: len(x))\n",
    "assert num_violations.equals(num_violations_nums)\n",
    "third_assump_test[1:1000].to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **ASSUMPTION 4**\n",
    "\n",
    "**GOOD ASSUMPTION**: with the previous preprocessing, all rows have comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_assump_test = third_assump_test.explode(['list_violations', 'v_num'])\n",
    "fourth_assump_test['list_violations'] = fourth_assump_test['list_violations'].str.lower()\n",
    "has_comments = fourth_assump_test['list_violations'].str.contains('- comments:')\n",
    "assert fourth_assump_test.shape[0] == has_comments.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CLEANED VIOLATIONS IN PRACTICE**\n",
    "Below is a function that can be used to clean the violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_violations(df: pd.DataFrame): \n",
    "  df['Violations'] = df['Violations'].str.lower()\n",
    "  mask = df['Violations'].notna()\n",
    "  # create violation number column\n",
    "  num_reg = r'(?:^|\\| )(\\d+\\. )'\n",
    "  vio_nums = df.loc[mask, 'Violations'].str.extractall(num_reg)\n",
    "  df.loc[mask, 'v_num'] = vio_nums.groupby(level=0)[0].apply(list)\n",
    "  # create violation_temp\n",
    "  split_reg = r'(?:^|\\| )\\d+\\. '\n",
    "  df.loc[mask, 'vio_temp'] = df.loc[mask, 'Violations'].str.split(split_reg, regex=True).apply(lambda x: x[1:])\n",
    "  # split into individual rows for violations\n",
    "  df = df.explode(['vio_temp', 'v_num']).reset_index(drop=True)\n",
    "  exp_mask = df['Violations'].notna()\n",
    "  # create comment and desc violation columns\n",
    "  desc_split_reg = '- comments:'\n",
    "  df.loc[exp_mask, 'v_comment'] = df.loc[exp_mask, 'vio_temp'].str.split(desc_split_reg).apply(lambda x: x[-1] if len(x) > 1 else np.nan)\n",
    "  df.loc[exp_mask, 'v_desc'] = df.loc[exp_mask, 'vio_temp'].str.split(desc_split_reg).apply(lambda x: x[0])\n",
    "  df = df.drop(columns=['vio_temp'])\n",
    "  return df\n",
    "\n",
    "clean_vios = clean_violations(food_ins_df)\n",
    "print(f'rows: {clean_vios.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **VIOLATIONS IN THE DATABASE**\n",
    "Additionally, the dirty database will show the three parts of the violation in one column; while the cleaned database will divide the three parts of the violations into three columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEFORE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = '''\n",
    "SELECT * \n",
    "FROM Inspections\n",
    "'''\n",
    "cur.execute(Q)\n",
    "df = pd.read_sql_query(Q, db_dirty)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AFTER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violations ---> Number, Comments, Description\n",
    "Q = '''\n",
    "SELECT * \n",
    "FROM Inspections\n",
    "'''\n",
    "cur.execute(Q)\n",
    "df = pd.read_sql_query(Q, db_clean)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dirty.close()\n",
    "db_clean.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('python_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2d609173ad082c7ff5cb3cd6dcfb139b58fd9f0168a432c49fd752c46a2c26f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
